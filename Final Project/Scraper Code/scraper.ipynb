{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evan Callia, Kyle Kusuda, Rachel Roberts\n",
    "\n",
    "This scraper scrapes the epa website containing a list of all the superfund zones in the United States. It grabs all relating information such as the name, id, indicator, etc. and then goes into the link it refrences in order to grab the address. Once all of the attribute data is grabbed and the location geocoded through nominatim, the list of superfund sites is output as a shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setting system pathways for arcmap\n",
    "import sys\n",
    "sys.path.append('C:\\\\Program Files (x86)\\\\ArcGIS\\\\Desktop10.3\\\\bin')\n",
    "sys.path.append('C:\\\\Program Files (x86)\\\\ArcGIS\\\\Desktop10.3\\\\arcpy')\n",
    "sys.path.append('C:\\\\Program Files (x86)\\\\ArcGIS\\\\Desktop10.3\\\\ArcToolbox\\\\Scripts') \n",
    "\n",
    "#importhing methods used for scraping\n",
    "import urllib2\n",
    "import lxml.html\n",
    "import requests\n",
    "import json\n",
    "import arcpy\n",
    "arcpy.env.workspace = r\"V:\\UW\\Geog 458\\Lab03\"\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "#importing method used to scrape javascript rendered website\n",
    "from selenium import webdriver\n",
    " \n",
    "driver = webdriver.Firefox()\n",
    "url = \"http://www.epa.gov/superfund/national-priorities-list-npl-sites-state#WA\"\n",
    "driver.get(url)\n",
    "\n",
    "dom = lxml.html.fromstring(driver.page_source)\n",
    "dom.make_links_absolute(url)\n",
    "\n",
    "#grab all places mentioned on website\n",
    "rows = dom.cssselect(\"table tbody tr\")\n",
    "\n",
    "#this will be the list of dictionaries containg each site and all of it's attributes\n",
    "#each dictionary will contain these elements: name, id, date, score, \n",
    "#indicator, link, city, state, and zipcode\n",
    "sites = []\n",
    "\n",
    "#keeps track of how many sites fail to geocode\n",
    "count = 0\n",
    "\n",
    "#go through each site and pull out attributes\n",
    "for row in rows:\n",
    "    #grab td elements (each containing one attribute)\n",
    "    atts = row.cssselect(\"td\")\n",
    "    name = atts[0].text\n",
    "    sid = atts[2].text\n",
    "    date = atts[3].text\n",
    "    score = atts[4].text\n",
    "    indic = atts[5].text\n",
    "    link = \"\"\n",
    "    address = \"\"\n",
    "            \n",
    "    #grab appropriate link\n",
    "    links = atts[6].cssselect(\"a\")\n",
    "    for templink in links:\n",
    "        if templink.text == \"Site Progress Profile\":\n",
    "            link = templink.attrib['href']\n",
    "\n",
    "            #go into link and pull out address components            \n",
    "            htmltwo = urllib2.urlopen(link).read()\n",
    "            domtwo = lxml.html.fromstring(htmltwo)\n",
    "            domtwo.make_links_absolute(link)\n",
    "        \n",
    "            comps = domtwo.cssselect(\"div#myrightbox br\")\n",
    "            if len(comps) > 4:\n",
    "                if comps[2].tail is not None:\n",
    "                    comp1 = comps[2].tail.strip()\n",
    "                else:\n",
    "                    comp1 = \"\"\n",
    "                if comps[3].tail is not None:\n",
    "                    comp2 = comps[3].tail.strip()\n",
    "                else:\n",
    "                    comp2 = \"\"\n",
    "                if comps[4].tail is not None:\n",
    "                    comp3 = comps[4].tail.strip()\n",
    "                else:\n",
    "                    comp3 = \"\"\n",
    "\n",
    "                temp = []\n",
    "                if \",\" in comp1:\n",
    "                    temp = comp1.split(' ')\n",
    "                elif \",\" in comp2:\n",
    "                    temp = comp2.split(' ')\n",
    "                else:\n",
    "                    temp = comp3.split(' ')\n",
    "\n",
    "                if len(temp) > 2:\n",
    "                    city = temp[len(temp)-3].strip() + \" \"\n",
    "                    state = temp[len(temp)-2].strip() + \" \"\n",
    "                    zipcode = temp[len(temp)-1].strip()\n",
    "\n",
    "                    sites.append({\"name\": name, \"site_id\": sid, \"date\": date, \"score\": score, \"indicator\": indic, \"link\": link, \"city\": city, \"state\": state, \"zipcode\": zipcode})\n",
    "                else:\n",
    "                    count += 1\n",
    "            else:\n",
    "                count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of our points to be made (populated later)\n",
    "points = []\n",
    "\n",
    "for site in sites:    \n",
    "    #concatinate url searches and retrieve data in json format\n",
    "    query = \"http://nominatim.openstreetmap.org/search?q=\" + site[\"city\"].replace(\" \", \"+\") + site[\"state\"].replace(\" \", \"+\") + site[\"zipcode\"].replace(\" \", \"+\") + \"&format=json&polygon=1&addressdetails=1\"\n",
    "    response = requests.get(query)\n",
    "    response_list = json.loads(response.content)\n",
    "    \n",
    "    #grab latitude and longitude from json returned\n",
    "    lat = \"\"\n",
    "    lon = \"\"\n",
    "    for row2 in response_list:\n",
    "        if \"lat\" in row2:\n",
    "            lat = row2[\"lat\"].strip()\n",
    "        if \"lon\" in row2:\n",
    "            lon = row2[\"lon\"].strip()\n",
    "            break\n",
    "        \n",
    "    #if score is invalid set to 9999\n",
    "    if site[\"score\"] is None:\n",
    "        site[\"score\"] = 9999\n",
    "    \n",
    "    #put the attributes and coordinates of each point in our points list\n",
    "    #check for exception and skip point if so (point failed to geocode)\n",
    "    try:\n",
    "        points.append((site[\"name\"], site[\"site_id\"], site[\"date\"], site[\"score\"], site[\"indicator\"], site[\"link\"], site[\"city\"], site[\"state\"], site[\"zipcode\"], [float(lon), float(lat)]))\n",
    "    except ValueError,e:\n",
    "        #print site[\"name\"] + \" Failed to geocode.\"\n",
    "        count += 1\n",
    "                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make feature class to put our points in (setting refrecences and adding fields needed)\n",
    "sr = arcpy.SpatialReference(4326)\n",
    "arcpy.CreateFeatureclass_management(r\"V:\\UW\\Geog 458\\458 Final Project\\Output\", \"superfund_sites.shp\", \"POINT\",\"\", \"\", \"\", sr)\n",
    "for key in sites[0]:\n",
    "    if key == \"date\":\n",
    "        arcpy.AddField_management(r\"V:\\UW\\Geog 458\\458 Final Project\\Output\\superfund_sites.shp\", str(key), \"DATE\", \"\", \"\", \"\", \"\", \"NULLABLE\")\n",
    "    elif key == \"score\":\n",
    "        arcpy.AddField_management(r\"V:\\UW\\Geog 458\\458 Final Project\\Output\\superfund_sites.shp\", str(key), \"DOUBLE\", \"\", \"\", \"\", \"\", \"NULLABLE\")\n",
    "    else:\n",
    "        arcpy.AddField_management(r\"V:\\UW\\Geog 458\\458 Final Project\\Output\\superfund_sites.shp\", str(key), \"TEXT\", \"\", \"\", \"\", \"\", \"NULLABLE\")\n",
    "                     \n",
    "#populate shapefile with our points\n",
    "cursor = arcpy.da.InsertCursor(r\"V:\\UW\\Geog 458\\458 Final Project\\Output\\superfund_sites.shp\", (\"name\", \"site_id\", \"date\", \"score\", \"indicator\", \"link\", \"city\", \"state\", \"zipcode\", \"SHAPE@XY\"))\n",
    "for point in points:\n",
    "    cursor.insertRow(point)\n",
    "del cursor\n",
    "\n",
    "print str(count) + \" sites failed to geocode.\"\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
